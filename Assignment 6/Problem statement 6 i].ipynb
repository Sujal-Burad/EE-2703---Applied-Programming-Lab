{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function optimization\n",
    "\n",
    "A common occurrence in several domains of engineering is that we need to optimize a certain function of some variables.  For example: $f(x) = x^2$.  Here we have a function of one variable ($x$) and we want to find the *minimum* value of that function over all values of $x$.  Of course, in this case we know that our objective is to minimize rather than maximize -- this may not always be the same option.\n",
    "\n",
    "This of course is precisely what Calculus is so good at: by setting the derivative of the function to 0, we can solve the equation to find the value of $x$ that gives a possible optimum value.  In the present case, $f'(x) = 2x \\Rightarrow x = 0$ will give us the *optimum* (in this case the minimum).\n",
    "\n",
    "But what if we wanted to do this the *hard* way - actually do it numerically.  In this case it may be the painful way, but in general it might be useful.  So let us first get the big picture - what does our function actually look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the imports\n",
    "%matplotlib ipympl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the axis and function\n",
    "xbase = np.linspace(-2, 2, 100)\n",
    "def yfunc(x):\n",
    "    return x ** 2\n",
    "ybase = yfunc(xbase)\n",
    "plt.plot(xbase, ybase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative improvement\n",
    "\n",
    "If we don't know how to optimize, one possible approach is to throw darts - make a random guess, and see how good the present value is.  If the new value is better than the old one, retain it, else make a new guess.  Keep doing this for a large number of guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some large value for the best cost found so far\n",
    "bestcost = 100000\n",
    "# Generate several values within a search 'space' and check whether the new value is better\n",
    "# than the best seen so far.\n",
    "bestx = -100\n",
    "rangemin, rangemax = -2, 2 \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xbase, ybase)\n",
    "xall, yall = [], []\n",
    "lnall,  = ax.plot([], [], 'ro')\n",
    "lngood, = ax.plot([], [], 'go', markersize=10)\n",
    "def onestep(frame):\n",
    "    global bestcost, bestx\n",
    "    # Generate a random value \\in -2, +2\n",
    "    x = np.random.random_sample() * 4 - 2\n",
    "    y = yfunc(x)\n",
    "    if y < bestcost:\n",
    "        # print(f\"Improved from {bestcost} at {bestx} to {y} at {x}\")\n",
    "        bestcost = y\n",
    "        bestx = x\n",
    "        lngood.set_data(x, y)\n",
    "    else:\n",
    "        # print(f\"New cost {y} worse than best so far: {bestcost}\")\n",
    "        pass\n",
    "    xall.append(x)\n",
    "    yall.append(y)\n",
    "    lnall.set_data(xall, yall)\n",
    "    # return lngood,\n",
    "\n",
    "ani= FuncAnimation(fig, onestep, frames=range(10), interval=1000, repeat=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic improvement\n",
    "\n",
    "Instead of randomly guessing values, is there a better way to proceed?  What if we know what kind of change in $x$ would result in a good change in $y$?  Can we *guide* the optimization process so it reaches the minimum in fewer steps?\n",
    "\n",
    "One way to do this is to recognize that at any point, the derivative of y can be used to guide the direction of improvement.  Let us say we are trying to minimize the function $f(x)$.\n",
    "\n",
    "$$\n",
    "f(x + dx) \\approx f(x) + f'(x) dx\n",
    "$$\n",
    "\n",
    "If $f'(x)>0$, the curve is sloping upwards as we increase $x$.  So we should go in the opposite direction, that is, decrease $x$.  Alternatively, if $f'(x)<0$, the function will decrease if we increase $x$, so we should increase $x$.  In other words, either way we should go in the opposite direction to $f'(x)$.\n",
    "\n",
    "### How much?\n",
    "\n",
    "So we know the direction, but how much should we change $x$?  If we change by a constant amount, then the change in $f(x)$ will be proportional to the value of $f'$ at that point.  But intuitively, it would seem that if there is a rapid change in the function, then it makes sense to take a larger step in the hope of reaching the optimum faster.  \n",
    "\n",
    "At the same time, too large a step could make us overshoot, and then we would have to come back.  \n",
    "\n",
    "Finally, a fixed step size would eventually get us to the right place, but it could take a long time if we are far away from the optimum, even though the gradient indicates that there is likely to be a large change in the function.\n",
    "\n",
    "This is captured using a parameter called the *learning rate*.  By modifying the learning rate, we can control how quickly we move towards the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some large value for the best cost found so far\n",
    "bestcost = 100000\n",
    "# Generate several values within a search 'space' and check whether the new value is better\n",
    "# than the best seen so far.\n",
    "bestx = 2\n",
    "rangemin, rangemax = -5, 5 \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xbase, ybase)\n",
    "xall, yall = [], []\n",
    "lnall,  = ax.plot([], [], 'ro')\n",
    "lngood, = ax.plot([], [], 'go', markersize=10)\n",
    "\n",
    "# Learning rate \n",
    "lr = 0.1\n",
    "\n",
    "def yprimefunc(x):\n",
    "    return 2 * x\n",
    "\n",
    "def onestepderiv(frame):\n",
    "    global bestcost, bestx, lr\n",
    "    x = bestx - yprimefunc(bestx) * lr \n",
    "    bestx = x\n",
    "    y = yfunc(x)\n",
    "    lngood.set_data(x, y)\n",
    "    xall.append(x)\n",
    "    yall.append(y)\n",
    "    lnall.set_data(xall, yall)\n",
    "    # return lngood,\n",
    "\n",
    "ani= FuncAnimation(fig, onestepderiv, frames=range(10), interval=1000, repeat=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple optima\n",
    "\n",
    "Consider the function $f(x) = x^4 - 2 x^3 + 3 x^2 + 5x +10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfunc(x):\n",
    "    return x**4 - 3*x**2 + 1*x \n",
    "\n",
    "def cfuncd(x):\n",
    "    return 4*x**3 -6*x + 1\n",
    "\n",
    "xbase = np.linspace(-2, 2, 100)\n",
    "ybase = cfunc(xbase)\n",
    "\n",
    "plt.close()\n",
    "plt.plot(xbase, ybase)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some large value for the best cost found so far\n",
    "bestcost = 100000\n",
    "# Generate several values within a search 'space' and check whether the new value is better\n",
    "# than the best seen so far.\n",
    "bestx = 0\n",
    "rangemin, rangemax = -5, 5 \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xbase, ybase)\n",
    "xall, yall = [], []\n",
    "lnall,  = ax.plot([], [], 'ro')\n",
    "lngood, = ax.plot([], [], 'go', markersize=10)\n",
    "\n",
    "# Learning rate \n",
    "lr = 0.1\n",
    "\n",
    "def onestepderiv(frame):\n",
    "    global bestcost, bestx, lr\n",
    "    x = bestx - cfuncd(bestx) * lr \n",
    "    bestx = x\n",
    "    y = cfunc(x)\n",
    "    lngood.set_data(x, y)\n",
    "    xall.append(x)\n",
    "    yall.append(y)\n",
    "    lnall.set_data(xall, yall)\n",
    "    # return lngood,\n",
    "\n",
    "ani= FuncAnimation(fig, onestepderiv, frames=range(10), interval=1000, repeat=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "- Implement a function that takes the following inputs, and finds the minimum using gradient descent\n",
    "  - Function definition (one variable)\n",
    "  - Derivative (also a function definition)\n",
    "  - Starting point\n",
    "  - Learning rate\n",
    "- Repeat the above, but with 2 or more variables (you will be tested with different functions with different numbers of variables depending on what you have implemented)\n",
    "- Use 3-D plots to show the path taken by a 2 variable optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "135c9c029983d30fe2c25215b219c39403965bc9bf9257a46b5d1e1e22d97d61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
